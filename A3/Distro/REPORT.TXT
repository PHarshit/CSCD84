CSC D84 - Artificial Intelligence

Assignment 3 - Reinforcement Learning - Q Learning

This assignment is worth:

15 AIUs (Artificial Intelligence Units)
toward the 35% assignment component of your final
mark.

________________________________________________

Student Name 1 (last, first): Mrigank Mehta

Student Name 2 (last, first): Huang Zhi Zhong

Student number 1: 1001309014

Student number 2: 1002671094

UTORid 1: mehtamri

UTORid 2: huang472

READ THIS AND SIGN YOUR NAME AT THE END:

 I certify that I have read the UTSC code on academic
honesty and plaguarism. All work submitted as part
of this assignment is my own.

	Signed: _Mrigank_Mehta_	_Zhi_Zhong_Huang_


(-5 marks for failing to provide the identifying
 information requested above)
________________________________________________

Answer the following questions. Be concise and clear
but explain carefully when needed.

1 .- (1 mark) Explain your reward function. What game elements
      are used, and why you think this is a good reward function
      for this problem.

	


2 .- These are multiple experiments (once you are sure your
     QLearning code is working!)

     * IMPORTANT! * For each training run, SAVE the Qtable
     you will re-use the Qtables later.

     (1 mark) # Experiment 1, 10000 training trials, 20 rounds, random seed 1522,
     # 8x8 grid, cat-smartness .9

     Initial mouse winning rate (first rate obtained when training starts): 6.0484%

     Train your mouse, and once trained, run the evaluation and
     record the mouse winning rate: 83.8225%

     (1 mark) # Experiment 2, 1000000 training trials, 50 rounds, random seed 1522,
     # 8x8 grid, cat-smartness .9

     Initial mouse winning rate (first rate obtained when training starts): 6.4436%

     Train your mouse, and once trained, run the evaluation and
     record the mouse winning rate: 90.4653%

     (1 mark) Would the mouse keep improving and become invincible if
     you ran, say, 100,000,000 training rounds per batch?

4 .- Using the QTable saved from Experiment 2 (NO re-training!)

     (1 mark) # Experiment 3 - Run the evaluation on the 8x8 grid, cat-smartness=.9,
     # but using the random seed 4289 for the game.

     Mouse Winning Rate: 33.3408%

     (2 mark) # Experiment 4 - Run the evaluation on the 8x8 grid, cat-smartness=.9,
     # but using the random seed 31415 for the game.

     Mouse Winning Rate: 47.4143%

     Average rate for Experiement 3 and Experiment 4: Average of Experiment 3 = 33.3165%, Average of Experiment 4 = 47.1141%

     (1 mark) Compare with the rate obtained in experiment 2. What is happening here?
	   A: The rate is a lot lower for Experiment 3 and Experiment 4, since the environment isn't static from Experiment 2, the one on which the mouse was actually trained.
	   In other words, since the environment layout changes the mouse doesn't know how to perform on it, and thus performs poorly.

5 .- Understanding the effects of state space on learning ability:

     (1 mark) # Experiment 5 - Train your mouse on the 16x16 grid, using random seed
     # 1522, 1000000 trials, and 50 training rounds. Cat-smartness=.9

     Initial mouse winning rate (first rate obtained when training starts): 3.0010%

     Mouse Winning Rate (from evaluation after training): 63.1559%

     (1 mark) Compare the final winning rate with that from Experiment 2, what can
     you say about the difference in winning rates?
     A: The winning rate is a lot lower for Experiment 5 compered to Experiment 2; this is because
     the state space is a lot larger for a 16x16 grid, compared to an 8x8 grid. The mouse needs
     to be trained a lot longer to achieve a winning rate similar to the one seen on the 8x8 grid.

6 .- (2 marks) Is standard Q-Learning a reasonable strategy for environments
     that change constantly? discuss based on the above

	A: It is not, as training using standard Q-Learning needs a static environment, as the states/size of the
	Q-Table remains constant regardless of the environment changing. As seen in the past experiments, when the 
	mouse is trained using standard Q-Learning in one seed, using the same table for another entirely new seed
	results in very poor performance of the mouse as the optimal action depending on state in one seed (where the mouse was first trained) 
	might be entirely different from the optimal action given an equivalent state in another seed.

7 .- (3 marks) Explain your feature set for feature-based Q Learning, and
               explain why and how each feature is expected to contribute
	       to helping your mouse win

	A: The features we've implemented typically have to do with minimizing the mouse's distance to the cheese
	and maximizing it's distance to any number of cats, and trying to minimize the ratio between cheese distance
	and cat distance.
	Feature 1 - closest cheese - helps mouse win by incentivizing mouse to move towards nearest cheese
  	feature 2 - closest cat - attempts to maximize distance from nearest cat so as to not be eaten and lose the game
  	feature 3 - reward based on difference between minimum cheese distance and minimum cat distance - attempts to give a reward 
	based on ratio between minimum cheese distance and minimum cat distance. This is so that the the mouse is incentivized to take
	actions which minimize cheese distance while also maximizing cat distance during the same move.
  	feature 4 - disincentivize dead-ends unless cheese is there - attempts to incentivize the mouse not to move into dead-ends in the
	maze and get itself trapped/stuck.
  	feature 5 - mean distance from mouse to all cats - attempt to maximize this so that the mouse avoids areas of high cat concentration/density
	(so it attempts to avoid these areas even when cheese is near/present at locations of high cat density)
  

8 .- Carry out the following experiments:

     (1 mark) # Experiment 6: Train your mouse using feature-based Q-Learning, on
     # the 16x16 grid, one cat, and one cheese, cat-smartness=.9,
     # random seed = 1522, and use 1000000 trials and 50 rounds.

     Initial mouse winning rate (first rate obtained when training starts): 3.3080%

     Mouse Winning Rate (from evaluation after training): 87.0781%

     Compare this rate against the rate obtained in Experiment #5, what
     are your conclusions regarding the utility of feature-based
     Q-Learning?

     (1 mark) # Experiment 7, using the saved weights from Experiment 6 (no
     # re-training) run the evaluation using random seed 4289, same
     # 1 cheese, 1 cat, and cat-smartness=.9, 16x16 grid

     Mouse Winning Rate (from evaluation after training): 86.4933%

     (1 mark) # Experiment 8, using the saved weights from Experiment 6 (no
     # re-training) run the evaluation using random seed 31415, same
     # 1 cheese, 1 cat, and cat-smartness=.9, 16x16 grid

     Mouse Winning Rate (from evaluation after training): 87.8494%

     (1 mark) Comparing the winning rates from Experiments #7 and #8 against
     those from Experiment #6, what can you conclude about feature-
     based Q-Learning in terms of its ability to deal with changes
     to the environment?

	A: Unlike standard Q-Learning, feature based Q-Learning allows the mouse to perform well
	in environments that it hasn't been trained in. As there isn't a QTable dependent on states 
	(only weights are modified and saved) it allows the mouse to perform the optimal action
	in unvisited/new states and new environments/seeds.
	

9 .- Carry out the following experiments:

     (1 mark) # Experiment 9: Train your mouse using feature-based Q-Learning, on
     # the 16x16 grid, 2 cats, and 3 cheeses, cat-smartness=.9,
     # random seed = 1522, and use 1000000 trials and 50 rounds.

     Initial mouse winning rate (first rate obtained when training starts): 8.6726%

     Mouse Winning Rate (from evaluation after training): 87.1393%

     (1 mark) # Experiment 9: Using the trained weights from Experiment 9, (no
     # re-training), run the evaluation on the 16x16 grid, 2 cats,
     # 3 cheeses, cat-smartness=.9, and random seed 4289

     Mouse Winning Rate (from evaluation after training): 86.2724%

     (1 mark) # Experiment 9: Using the trained weights from Experiment 9, (no
     # re-training), run the evaluation on the 32x32 grid, 2 cats,
     # 3 cheeses, cat-smartness=.9, and random seed 4289

     Mouse Winning Rate (from evaluation after training): 88.9885%

     (2 marks) Based on the last 3 experiments, what can you conclude regarding
     the usefulness and applicability of standard Q-Learning vs.
     feature-based Q-learning?

10 . - (2 marks) We obviously can not train an expensive robot by trial and error,
      How would you implement feature-based Q-learning for a mobile bot so you
      can take advantage of reinforcement learning but not spend millions on
      broken robots that keep falling down the stairs?

	A: Train the robot in a controlled environment that introduces it to a series of different environmental factors (such as differences in height, uneven terrain, etc). 
This will allow the robot to take into account many different factors (based on the features we've defined) while also being relatively safe from harm/damage as it's being trained in a controlled environment.
As feature based Q-Learning is somewhat environment independent (as specific factors in the environment still have to be specified by the programmer/person training the robot as features) introducing it to an uncontrolled environment after being trained still results in it performing successfully, as the robot can respond properly to a non-static environment due to feature based Q-Learning training the robot in a more generalized way (i.e it can respond to states it's never visited before, in contrast to standard Q-Learning). 

_____________________________________________________

Mark with an 'x' where appropriate. If something is only
working partially, briefly describe what works, what
doesn't work, or what problems exist.

			                 Complete/Working	   Partial		Not done

QLearn
 update                      [x]

Reward
 function                    [x]

Decide
 action                      [x]

featureEval                  [x]

evaluateQsa                  [x]

maxQsa_prime                 [x]

Qlearn_features              [x]

decideAction_features        [x]

_____________________________________________________

Marking:

(10 marks) Implemented QLearn update.

(5 marks) Implemented a reasonable reward function

(5 marks)  Implemented the function that chooses the
           optimal action based on Qtable and
           maze

(15 marks) Implemented a non-trivial, good feature set
	   and evaluation function

(20 marks) Implemented a working feature-based Q-learning
	   algorithm

(20 marks) Competitive mouse performance

(25 marks) Answers in this report file

(- marks)  Penalty marks

Total for A3:       / out of 100
